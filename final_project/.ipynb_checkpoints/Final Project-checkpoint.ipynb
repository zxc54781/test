{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> 主題：英國AI跳棋 </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./image/03.png\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">題目簡介</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用py.game完成英國跳棋的遊戲程式，<br><br>一開始使用alpha-beta pruning來進行運算最佳棋步，<br><br>殘局的部分則使用Deep Q-learning Network (DQN)完成收拾。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./image/Demo.png\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">排程</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  05/22 : 編寫完成py.game：英國跳棋<br><br>05/28 : 編寫alpha-beta pruning並測試<br><br>05/31 : 與其他跳棋遊戲AI PK<br><br>06/02 : 加入殘局收拾網路(creat by DQN)<br><br>06/04 : Training it !<br><br>06/05 : 統整專案並編寫jupyter notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">alpha-beta pruning </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> α-β pruning 是由Minimax Algorithm演變而來，所以我們要先從Minimax Algorithm說明。</h3>\n",
    "\n",
    "<h2> Minimax Algorithm</h2>\n",
    "\n",
    "\n",
    "<h3> Minimax Algorithm主要是用在回合制的遊戲，將所有可能的步驟求出並加以評分，<br>由於雙方目標不同分別為讓自己得最高分以及讓對方拿最低分，導致每個回合的所求相反，<br>如Figure1。<br><br>但是此方法有一個很明顯的缺點，計算量會非常龐大，可能局面亦會很多，如Figure2所示為當棋子數目給定時之所有可能局面。</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./image/Minimax.png\" width=600 height=600  /><a style=\"float:left;\">\n",
    "<img src=\"./image/number.png\" width=250 height=200 style=\"position:relative;left: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure1:α-β pruning示意圖。左手邊數字為回合數，而括號內文字為該局所求，圓形及方型中數字則為該State之Reward</h6>\n",
    "<h6>Figure2:棋子數與可走步驟圖。</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> α-β pruning</h2>\n",
    "\n",
    "<h3> α-β pruning 最重要的改變就是將一些不用算的狀態(state)給刪減掉，這樣對減少計算有極顯著的效益！<br><br>步驟一：先選取其中一個步驟\n",
    "並計算至選定的深度層。<br><br>步驟二：向上一個節點把節點下的數給算出來，並觀察該節點需求是取最大還是最小，取最小則改變該節點之β，反之亦然。<br><br>步驟三：再往上一個節點重複步驟一，如果該節點取得比同母節點之節點β(或α)小(或大)則旁枝則可不計算。<br><br>詳見figure3或reference3。<br></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./image/gif.gif\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure3:α-β purning流程圖，α預設值為：∞ ； β預設為：-∞，方形的目標為找最大值，圓形則為找最小值。</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Deep Q-learning Network ( DQN ) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Deep Q-learning Network 是基於 Q-learning的基礎發展出來的一種Reinforced  Learning(強化學習)的方法，<br>接下來會對Q-learning做簡單的介紹。</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Q-learning </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>我們須先將所有可能的狀態及動作列出並創建出Ｑtabel，接著將當前的狀態 ( State ) 輸入 Agent 計算後會利用Ｑtable在所有動作中選擇一個回報 ( Reward ) 最大的動作，，並作用在環境使狀態發生改變 ( State(0)=>State(1) )，但如果是很多可能的state ( 像是以照片的形式作為輸入 ) 就可能會造成維度災難！</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./image/prossesion.jpg\" width=400 height=400  /><a style=\"float:left;\">\n",
    "<img src=\"./image/prosseison2.png\" width=500 height=400 style=\"position:relative;left: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure4:Q-learning流程中英文對照圖<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./image/eq.jpg\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure5:Q-learning決策方程式<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> DQN </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>要解決維度災難最簡單的方法就是不要用Ｑtable或是盡量將輸入的狀態 ( State )降為成為維度更低特徵 ( feature )，其中ＤＱＮ就是以Network取代了Ｑtable的方式，不需要再將所有的State列出來，而是只需要將當前的狀態 ( State ) 輸入network則就會out put 相應的動作及所獲的的reward，在由中選取最大的，去作用在環境上，使狀態發生改變 ( State(0)=>State(1) )。其中與Q learning最不同的就是ＤＱＮ的訓練資料是來自於自己的，他會有兩個network，一個為evaluate network ( 估計網路 ) 時時刻刻更新，另外一個為 target network ( 固定網路 ) 一段時間更新一次並把權重在丟到evaluate network，簡單來說就是以自己產生的參數訓練自己</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:center;\"><img src=\"./image/vs.png\" width=600 height=600  /><a style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure6:Q-learning 與 DQN 之比較<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a style=\"float:left;\"><img src=\"./image/dqn.png\" width=500 height=400  /><a style=\"float:left;\">\n",
    "<img src=\"./image/dqn2.png\" width=400 height=400 style=\"position:relative;left: 50px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Figure7:DQN流程與中文對應圖<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">結果與討論</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Code </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align:center;\">Reference</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center;\">原理部分</h3><br><br>\n",
    "<h6>1.Minimax : https://en.wikipedia.org/wiki/Minimax </h6>\n",
    "<h6>2.J.Schaeffer et al. , <l>Checkers Is Solved</l> , Science , Vol. 317, Issue 5844 , pp. 1518-1522 , Sep . 2007</h6>\n",
    "<h6>3.CS 161 Recitation Notes - Minimax with Alpha Beta Pruning : https://pse.is/HSWFF</h6>\n",
    "<h6>4.强化学习:Q-learning由浅入深 : https://zhuanlan.zhihu.com/p/35724704</h6>   \n",
    "<h6>5.DQN从入门到放弃 : https://zhuanlan.zhihu.com/p/21421729 </h6>\n",
    "<h6>6.An introduction to Deep Q-Learning: let’s play Doom : https://reurl.cc/7N13D</h6>\n",
    "<h6>7.强化学习—DQN算法原理详解 : https://wanjun0511.github.io/2017/11/05/DQN/ </h6>\n",
    "<h6>8.李宏毅老師DRL Lecture：https://reurl.cc/bgOYr</h6><br><br>\n",
    "<h3 style=\"text-align:center;\">程式碼部分(DQN)</h3><br><br>\n",
    "<h6>9.莫煩 PYHTON : https://morvanzhou.github.io<h6>\n",
    "<h6>10.ReinforcementLearning_by_keras : https://github.com/Jason33Wang/ReinforcementLearning_by_keras<h6>\n",
    "<h6>11.Reinforcement-Learning-five-in-a-row: https://github.com/zhijs/-Reinforcement-Learning-five-in-a-row-<h6><br>\n",
    "<h3 style=\"text-align:center;\">程式碼部分(棋盤)</h3><br><br>\n",
    "<h6>12.Deep reinforcement learning for checkers -- pretraining a policy : https://reurl.cc/8RgEd</h6>\n",
    "<h6>13.PyCheckers : https://reurl.cc/D7VRO</h6>\n",
    "<h6>14.Checkers-Reinforcement-Learning : https://reurl.cc/Mglan </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

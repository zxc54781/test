{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9b62b0534ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_variables_to_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict, namedtuple\n",
    "from config import *\n",
    "from utils import *\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    _MODEL_DEF_TEMPLATE = {'batch_size': 1, 'dropout': False, 'init': None}\n",
    "\n",
    "    def __init__(self, name, load_dir, trainable, device):\n",
    "        self.name = name\n",
    "        self.trainable = trainable\n",
    "        self.load_dir = load_dir\n",
    "        self.write_dir = os.path.join(POLICY_PATH, 'tmp/')\n",
    "        self._vars = []\n",
    "        self._ops = []\n",
    "        self.scopes = []\n",
    "        self._device = device\n",
    "\n",
    "    def _build_graph(self):\n",
    "        pass\n",
    "\n",
    "    def _constructor(self):\n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            try:\n",
    "                with tf.device(self._device):\n",
    "                    self._build_graph()\n",
    "            except AttributeError:\n",
    "                self._build_graph()\n",
    "\n",
    "    def init(self, session, load_dir=None):\n",
    "        load_dir = load_dir or self.load_dir\n",
    "        for i, layer_scope in enumerate(self.scopes):\n",
    "            vars_to_load = [v for v in tf.global_variables() if\n",
    "                            v.name.startswith(layer_scope)]\n",
    "            self._vars.extend(vars_to_load)\n",
    "            session.run(tf.variables_initializer(var_list=vars_to_load))\n",
    "            if load_dir and vars_to_load:\n",
    "                saver = tf.train.Saver(vars_to_load)\n",
    "                saver.restore(session, os.path.join(load_dir, self.name))\n",
    "                # import pprint\n",
    "                # pp = pprint.PrettyPrinter(indent=3)\n",
    "                # info('Successfully loaded from {}:'.format(load_dir))\n",
    "                # pp.pprint(vars_to_load)\n",
    "\n",
    "    def save_params(self, session, step=None):\n",
    "        assert self.write_dir\n",
    "        info('Saving {0} to {1}'.format(self.name, self.write_dir))\n",
    "        saver = tf.train.Saver(list(set(self._vars)))\n",
    "        saver.save(session, os.path.join(self.write_dir, self.name), global_step=step)\n",
    "\n",
    "    def save_graph(self, session:tf.Session, fname:str, var_names:list):\n",
    "        frozen_graph = convert_variables_to_constants(session, session.graph_def, var_names)\n",
    "        tf.train.write_graph(frozen_graph, self.write_dir, fname + '.pb', as_text=False)\n",
    "        tf.train.write_graph(frozen_graph, self.write_dir, fname + '.txt', as_text=True)\n",
    "\n",
    "\n",
    "class Policy(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 session:tf.Session,\n",
    "                 name=None,\n",
    "                 load_dir=None,\n",
    "                 trainable=False,\n",
    "                 selection='greedy',\n",
    "                 device='GPU'):\n",
    "        super().__init__(name=\"POLICY_{}\".format(name),\n",
    "                         load_dir=load_dir,\n",
    "                         trainable=trainable,\n",
    "                         device=device)\n",
    "        self.session = session\n",
    "        self.state = tf.placeholder(dtype=tf.int32,\n",
    "                                    shape=(None, 8, 4),\n",
    "                                    name=\"state\")\n",
    "        self.action_label = tf.placeholder(dtype=tf.int32,\n",
    "                                           shape=(None, 128),\n",
    "                                           name=\"action\")\n",
    "        self.selection = selection\n",
    "        self._constructor()\n",
    "\n",
    "    def _build_graph(self):\n",
    "\n",
    "        #################### Graph inputs ####################\n",
    "        self.batch_size = tf.placeholder(shape=(), dtype=tf.float32, name=\"batch_size\")\n",
    "        self.keep_prob = tf.placeholder(shape=(),\n",
    "                                        dtype=tf.float32,\n",
    "                                        name=\"keep_prob\") if self.trainable \\\n",
    "                    else tf.constant(value=1,\n",
    "                                     dtype=tf.float32,\n",
    "                                     name=\"keep_prob\")\n",
    "        self.lr = tf.placeholder(shape=(), dtype=tf.float32, name=\"learning_rate\")\n",
    "        self.adv = tf.placeholder(shape=(None), dtype=tf.float32, name=\"advantage\")\n",
    "\n",
    "        ##################### Data layer #####################\n",
    "        X = tf.expand_dims(tf.cast(self.state, tf.float32), axis=3)\n",
    "\n",
    "        ###################### Inception #####################\n",
    "        with tf.variable_scope(\"INCEPTION\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            for i, (ksizes, nkernels) in enumerate(zip(KERNEL_SIZES, N_KERNELS)):\n",
    "                conv = []\n",
    "                for ks, nk in zip(ksizes, nkernels):\n",
    "                    w = tf.get_variable(shape=[ks[0], ks[1], X.shape[-1], nk],\n",
    "                                        initializer=PARAM_INIT,\n",
    "                                        trainable=self.trainable,\n",
    "                                        name='incep_{0}_w_K{1}{2}'.format(i + 1, ks[0], ks[1]))\n",
    "                    b = tf.get_variable(shape=[nk],\n",
    "                                        initializer=PARAM_INIT,\n",
    "                                        trainable=self.trainable,\n",
    "                                        name='incep_{0}_b_K{1}{2}'.format(i + 1, ks[0], ks[1]))\n",
    "                    c = tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "                    z = INCEP_ACT(c + b)\n",
    "                    conv.append(z)\n",
    "                X = tf.concat(conv, axis=3)\n",
    "            X = tf.nn.dropout(X, keep_prob=self.keep_prob)\n",
    "\n",
    "        ####################### Flatten ######################\n",
    "        conv_out = tf.contrib.layers.flatten(inputs=X)\n",
    "        X = conv_out\n",
    "        hwy_size = X.shape[-1]\n",
    "\n",
    "        ####################### Highway ######################\n",
    "        with tf.variable_scope(\"HIGHWAY\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            for i in range(HWY_LAYERS):\n",
    "                with tf.variable_scope('HWY_{}'.format(i)):\n",
    "                    wh = tf.get_variable(shape=[hwy_size, hwy_size],\n",
    "                                         initializer=PARAM_INIT,\n",
    "                                         trainable=self.trainable,\n",
    "                                         dtype=tf.float32,\n",
    "                                         name=\"hwy_w_{}\".format(i + 1))\n",
    "                    bh = tf.get_variable(shape=[hwy_size],\n",
    "                                         initializer=PARAM_INIT,\n",
    "                                         trainable=self.trainable,\n",
    "                                         dtype=tf.float32,\n",
    "                                         name=\"hwy_b_{}\".format(i + 1))\n",
    "                    wt = tf.get_variable(shape=[hwy_size, hwy_size],\n",
    "                                         initializer=PARAM_INIT,\n",
    "                                         trainable=self.trainable,\n",
    "                                         dtype=tf.float32,\n",
    "                                         name=\"T_w_{}\".format(i + 1))\n",
    "                    T = tf.sigmoid(tf.matmul(X, wt) + HWY_BIAS)\n",
    "                    H = tf.nn.relu(tf.matmul(X, wh) + bh)\n",
    "                    X = T * H + (1.0 - T) * X\n",
    "            X = tf.nn.dropout(X, keep_prob=self.keep_prob)\n",
    "            X = tf.concat([X, conv_out], axis=1)\n",
    "\n",
    "        ####################### Output #######################\n",
    "        with tf.variable_scope(\"OUTPUT\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            w = tf.get_variable(shape=[X.shape[-1], 128],\n",
    "                                initializer=PARAM_INIT,\n",
    "                                trainable=self.trainable,\n",
    "                                dtype=tf.float32,\n",
    "                                name=\"w_logit\")\n",
    "            b = tf.get_variable(shape=[128],\n",
    "                                initializer=PARAM_INIT,\n",
    "                                trainable=self.trainable,\n",
    "                                dtype=tf.float32,\n",
    "                                name=\"b_logit\")\n",
    "            self.logits = tf.add(tf.matmul(X, w), b, name=\"policy_logits\")\n",
    "            self.softmax = tf.nn.softmax(logits=self.logits, axis=1, name=\"policy_softmax\")\n",
    "            self.action = tf.argmax(input=self.softmax, axis=1, name=\"action\")\n",
    "            self.probs, self.actions = tf.nn.top_k(input=self.softmax, k=128, sorted=True)\n",
    "\n",
    "        ####################### Metrics ######################\n",
    "        with tf.variable_scope(\"METRICS\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            self.top_1_acc = tf.metrics.accuracy(labels=tf.argmax(self.action_label, axis=1),\n",
    "                                                 predictions=self.action,\n",
    "                                                 name=\"accuracy\")\n",
    "            self.top_2_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 2), tf.float32))\n",
    "            self.top_3_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 3), tf.float32))\n",
    "            self.top_5_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 5), tf.float32))\n",
    "            self.top_10_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 10), tf.float32))\n",
    "\n",
    "        ###################### Optimizer #####################\n",
    "        if self.trainable:\n",
    "            with tf.variable_scope(\"LOSS\", reuse=False) as scope:\n",
    "                self.scopes.append(scope.name)\n",
    "                self.step = tf.Variable(0, trainable=False)\n",
    "                self.reg_loss = LAMBDA * tf.add_n(\n",
    "                    [tf.nn.l2_loss(v) for v in tf.global_variables() if v.name.__contains__(\"w_\")])\n",
    "                self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                        labels=self.action_label,\n",
    "                        logits=self.logits,\n",
    "                        name=\"cross_entropy\")\n",
    "                self.loss1 = tf.add(self.reg_loss, self.cross_entropy, name=\"loss1\")\n",
    "                self.optimizer1 = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                         name=\"optimizer_pretrain\")\n",
    "                self.grad_update1 = self.optimizer1.minimize(\n",
    "                    loss=self.loss1,\n",
    "                    var_list=tf.trainable_variables(),\n",
    "                    global_step=self.step,\n",
    "                    name=\"grad_update\")\n",
    "                self.gradlogprob_adv = self.adv * tf.log(self.softmax)\n",
    "                self.pg_loss = tf.reduce_mean(input_tensor=-self.gradlogprob_adv,\n",
    "                                              axis=1,\n",
    "                                              name=\"pg_loss\")\n",
    "                self.optimizer2 = tf.train.RMSPropOptimizer(learning_rate=self.lr,\n",
    "                                                            decay=0.99,\n",
    "                                                            epsilon=1e-5)\n",
    "                self.policy_update = self.optimizer2.apply_gradients(\n",
    "                    grads_and_vars=[self.pg_loss, self.vars],\n",
    "                    global_step=self.step)\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return [v for v in tf.trainable_variables() if\n",
    "                v.name.lower().__contains__(self.name.lower())]\n",
    "\n",
    "\n",
    "\n",
    "class Value(Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 session:tf.Session,\n",
    "                 name=None,\n",
    "                 load_dir=None,\n",
    "                 trainable=False,\n",
    "                 selection='greedy',\n",
    "                 device='GPU'):\n",
    "        super().__init__(name=\"VALUE_{}\".format(name), load_dir=load_dir, trainable=trainable, device=device)\n",
    "        self.session = session\n",
    "        self.state = tf.placeholder(dtype=tf.int32,\n",
    "                                    shape=(None, 8, 4),\n",
    "                                    name=\"state\")\n",
    "        self.action_label = tf.placeholder(dtype=tf.int32,\n",
    "                                           shape=(None, 128),\n",
    "                                           name=\"action\")\n",
    "        self.selection = selection\n",
    "        self._constructor()\n",
    "\n",
    "    def _build_graph(self):\n",
    "\n",
    "        #################### Graph inputs ####################\n",
    "        self.batch_size = tf.placeholder(shape=(), dtype=tf.float32, name=\"batch_size\")\n",
    "        self.keep_prob = tf.placeholder(shape=(),\n",
    "                                        dtype=tf.float32,\n",
    "                                        name=\"keep_prob\") if self.trainable \\\n",
    "                    else tf.constant(value=1,\n",
    "                                     dtype=tf.float32,\n",
    "                                     name=\"keep_prob\")\n",
    "        self.lr = tf.placeholder(shape=(), dtype=tf.float32, name=\"learning_rate\")\n",
    "        self.adv = tf.placeholder(shape=(None), dtype=tf.float32, name=\"advantage\")\n",
    "\n",
    "        ##################### Data layer #####################\n",
    "        X = tf.expand_dims(tf.cast(self.state, tf.float32), axis=3)\n",
    "\n",
    "        ###################### Inception #####################\n",
    "        with tf.variable_scope(\"INCEPTION\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            for i, (ksizes, nkernels) in enumerate(zip(KERNEL_SIZES, N_KERNELS)):\n",
    "                conv = []\n",
    "                for ks, nk in zip(ksizes, nkernels):\n",
    "                    w = tf.get_variable(shape=[ks[0], ks[1], X.shape[-1], nk],\n",
    "                                        initializer=PARAM_INIT,\n",
    "                                        trainable=self.trainable,\n",
    "                                        name='incep_{0}_w_K{1}{2}'.format(i + 1, ks[0], ks[1]))\n",
    "                    b = tf.get_variable(shape=[nk],\n",
    "                                        initializer=PARAM_INIT,\n",
    "                                        trainable=self.trainable,\n",
    "                                        name='incep_{0}_b_K{1}{2}'.format(i + 1, ks[0], ks[1]))\n",
    "                    c = tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "                    z = INCEP_ACT(c + b)\n",
    "                    conv.append(z)\n",
    "                X = tf.concat(conv, axis=3)\n",
    "            X = tf.nn.dropout(X, keep_prob=self.keep_prob)\n",
    "\n",
    "        ####################### Flatten ######################\n",
    "        conv_out = tf.contrib.layers.flatten(inputs=X)\n",
    "        X = conv_out\n",
    "        hwy_size = X.shape[-1]\n",
    "\n",
    "        ####################### Highway ######################\n",
    "        with tf.variable_scope(\"HIGHWAY\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            for i in range(HWY_LAYERS):\n",
    "                with tf.variable_scope('HWY_{}'.format(i)):\n",
    "                    wh = tf.get_variable(shape=[hwy_size, hwy_size],\n",
    "                                         initializer=PARAM_INIT,\n",
    "                                         trainable=self.trainable,\n",
    "                                         dtype=tf.float32,\n",
    "                                         name=\"hwy_w_{}\".format(i + 1))\n",
    "                    bh = tf.get_variable(shape=[hwy_size],\n",
    "                                         initializer=PARAM_INIT,\n",
    "                                         trainable=self.trainable,\n",
    "                                         dtype=tf.float32,\n",
    "                                         name=\"hwy_b_{}\".format(i + 1))\n",
    "                    wt = tf.get_variable(shape=[hwy_size, hwy_size],\n",
    "                                         initializer=PARAM_INIT,\n",
    "                                         trainable=self.trainable,\n",
    "                                         dtype=tf.float32,\n",
    "                                         name=\"T_w_{}\".format(i + 1))\n",
    "                    T = tf.sigmoid(tf.matmul(X, wt) + HWY_BIAS)\n",
    "                    H = tf.nn.relu(tf.matmul(X, wh) + bh)\n",
    "                    X = T * H + (1.0 - T) * X\n",
    "            X = tf.nn.dropout(X, keep_prob=self.keep_prob)\n",
    "            X = tf.concat([X, conv_out], axis=1)\n",
    "\n",
    "        ####################### Output #######################\n",
    "        with tf.variable_scope(\"OUTPUT\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            w = tf.get_variable(shape=[X.shape[-1], 128],\n",
    "                                initializer=PARAM_INIT,\n",
    "                                trainable=self.trainable,\n",
    "                                dtype=tf.float32,\n",
    "                                name=\"w_logit\")\n",
    "            b = tf.get_variable(shape=[128],\n",
    "                                initializer=PARAM_INIT,\n",
    "                                trainable=self.trainable,\n",
    "                                dtype=tf.float32,\n",
    "                                name=\"b_logit\")\n",
    "            self.logits = tf.add(tf.matmul(X, w), b, name=\"policy_logits\")\n",
    "            self.softmax = tf.nn.softmax(logits=self.logits, axis=1, name=\"policy_softmax\")\n",
    "            self.action = tf.argmax(input=self.softmax, axis=1, name=\"action\")\n",
    "            self.probs, self.actions = tf.nn.top_k(input=self.softmax, k=128, sorted=True)\n",
    "\n",
    "        ####################### Metrics ######################\n",
    "        with tf.variable_scope(\"METRICS\", reuse=False) as scope:\n",
    "            self.scopes.append(scope.name)\n",
    "            self.top_1_acc = tf.metrics.accuracy(labels=tf.argmax(self.action_label, axis=1),\n",
    "                                                 predictions=self.action,\n",
    "                                                 name=\"accuracy\")\n",
    "            self.top_2_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 2), tf.float32))\n",
    "            self.top_3_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 3), tf.float32))\n",
    "            self.top_5_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 5), tf.float32))\n",
    "            self.top_10_acc = tf.reduce_mean(\n",
    "                tf.cast(tf.nn.in_top_k(self.softmax, tf.argmax(self.action_label, axis=1), 10), tf.float32))\n",
    "\n",
    "        ###################### Optimizer #####################\n",
    "        if self.trainable:\n",
    "            with tf.variable_scope(\"LOSS\", reuse=False) as scope:\n",
    "                self.scopes.append(scope.name)\n",
    "                self.step = tf.Variable(0, trainable=False)\n",
    "                self.reg_loss = LAMBDA * tf.add_n(\n",
    "                    [tf.nn.l2_loss(v) for v in tf.global_variables() if v.name.__contains__(\"w_\")])\n",
    "                self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                        labels=self.action_label,\n",
    "                        logits=self.logits,\n",
    "                        name=\"cross_entropy\")\n",
    "                self.loss1 = tf.add(self.reg_loss, self.cross_entropy, name=\"loss1\")\n",
    "                self.optimizer1 = tf.train.AdamOptimizer(learning_rate=self.lr,\n",
    "                                                         name=\"optimizer_pretrain\")\n",
    "                self.grad_update1 = self.optimizer1.minimize(\n",
    "                    loss=self.loss1,\n",
    "                    var_list=tf.trainable_variables(),\n",
    "                    global_step=self.step,\n",
    "                    name=\"grad_update\")\n",
    "                self.neg_grad_log_prob_adv = self.adv * -tf.log(self.softmax)\n",
    "                self.SFGE = tf.reduce_mean(input_tensor=self.neg_grad_log_prob_adv,\n",
    "                                           axis=1,\n",
    "                                           name=\"score_func_grad_estimator\")\n",
    "\n",
    "\n",
    "\n",
    "class A2CLoss:\n",
    "\n",
    "    def __init__(self, policy_network, value_network):\n",
    "        self.policy = policy_network\n",
    "        self.value = value_network\n",
    "        self.__build_graph()\n",
    "\n",
    "    def __build_graph(self):\n",
    "        with tf.variable_scope(\"AC2_LOSS\", reuse=False) as scope:\n",
    "            self.policy.scopes.append(scope.name)\n",
    "            self.value.scopes.append(scope.name)\n",
    "            self.lrate = tf.placeholder(shape=(), dtype=tf.float32, name=\"lrate\")\n",
    "            self.rewards = tf.placeholder(shape=(None), dtype=tf.float32, name=\"rewards\")\n",
    "            self.baseline = tf.placeholder(shape=(None), dtype=tf.float32, name=\"value_estimate\")\n",
    "            self.gradlogp = self.adv * tf.log(self.policy.softmax)\n",
    "            self.logprob = tf.reduce_mean(input_tensor=-self.gradlogprob_adv,\n",
    "                                          axis=1,\n",
    "                                          name=\"pg_loss\")\n",
    "            self.policy_entropy = -tf.reduce_sum(\n",
    "                self.policy.softmax * tf.log(self.policy.softmax), axis=1\n",
    "            )\n",
    "\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.lr,\n",
    "                                                       decay=0.99,\n",
    "                                                       epsilon=1e-5)\n",
    "            self.policy_update = self.optimizer.apply_gradients(\n",
    "                grads_and_vars=[self.pg_loss, self.policy.vars],\n",
    "                global_step=self.policy.step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
